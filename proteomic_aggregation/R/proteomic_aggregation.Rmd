# Proteomic Data Aggregation and Visualization


This notebook demonstrates aggregation of proteomic data via the National Microbiome Data Collaborative (NMDC)'s [Runtime API](https://api.microbiomedata.org/docs). It highlights how the NMDC's schema can be used to overcome some of the numerous challenges associated with this type of aggregation. Please note that this notebook is intended for individuals with experience performing mass spectrometry based proteomic analyses and that various parameter and processing choices were made for this example use case. They are not broadly applicable and should be adjusted as needed. 


Notebook Steps:

1) Assess background information and collect datasets for an example study of riverbed sediment along the Columbia River

2) Apply a spectral probability filter across the data that optimizes the number of identifications for an FDR of 0.05

3) Collapse to unique peptides and normalize quantification

4) Extract functional gene annotations for proteins

5) Generate annotation and protein mappings for peptides using "Razor" strategy

6) Perform protein rollup using the "Razor" results and summarize into an aggregated table of relative protein abundance



Import libraries and python scripts containing functions necessary to run this notebook. 'aggregation_functions.py' (also in this folder) includes spectral probability filtering and protein mapping functions. 'nmdc_api.py' (located in NOM_visualization/python) includes functions for API traversal of the collections endpoint.

```{r}
# import requests
# from io import StringIO
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# import sys
# from IPython.display import Image, display
# 
# if 'google.colab' in sys.modules:
# 
#    #module in this folder with specific protein aggregation functions
#    !wget https://raw.githubusercontent.com/microbiomedata/nmdc_notebooks/refs/heads/main/proteomic_aggregation/python/aggregation_functions.py
#    import aggregation_functions as agg_func
# 
#    #pmartR logo
#    !wget https://raw.githubusercontent.com/microbiomedata/nmdc_notebooks/refs/heads/main/proteomic_aggregation/pmartR_logo_final.jpg
# 
#    #module for API functions currently residing in NOM_visualizations
#    !wget https://raw.githubusercontent.com/microbiomedata/nmdc_notebooks/refs/heads/main/NOM_visualizations/python/nmdc_api.py
#    import nmdc_api as api_func
# 
# else:
#   #module in this folder with specific protein aggregation functions
#   import aggregation_functions as agg_func
# 
#   #module for API functions currently residing in NOM_visualizations
#   api_func=agg_func.import_relative_module("nmdc_api", "../../NOM_visualizations/python/nmdc_api.py")




# Setup 
# Add renv project library to R environment variable libPaths()
.libPaths(c(.libPaths(), "../../renv/library/*/R-*/*"))

# Load required packages
suppressPackageStartupMessages({
  library(dplyr, warn.conflicts = FALSE)
  library(tidyr, warn.conflicts = FALSE)
  library(stringr, warn.conflicts = FALSE)
  library(readr, warn.conflicts = FALSE)
  library(ggplot2, warn.conflicts = FALSE)
  library(jsonlite, warn.conflicts = FALSE)
  library(janitor, warn.conflicts = FALSE)
  library(grid, warn.conflicts = FALSE)
  })

# Load NMDC API functions from this repo
if(Sys.getenv("COLAB_BACKEND_VERSION") == "") {
  source("../../utility_functions.R")
  # import aggregation functions script
}

if(Sys.getenv("COLAB_BACKEND_VERSION") != "") {
  source("http://raw.githubusercontent.com/microbiomedata/nmdc_notebooks/refs/heads/main/utility_functions.R")
  # import aggregation functiosn script
}
     

  
```

## 1. Assess background information and collect data for an example study of riverbed sediment along the Columbia River

Review the example study on the [NMDC data portal](https://data.microbiomedata.org/details/study/nmdc:sty-11-aygzgv51). Use the study `id` embedded in the url (nmdc:sty-11-aygzgv51) to collect all related data objects via the [NMDC Runtime API](https://api.microbiomedata.org/docs) and reformat the json output into a pandas dataframe. These data objects reference both input files (i.e. raw, gff) and output files (i.e. metaproteomic results) to the NMDC workflows.

```{r} 
# pd.set_option("display.max_rows", 6)
# 
# #get all data objects associated with this study id using the NMDC find endpoint
# og_url = f'https://api.microbiomedata.org/data_objects/study/nmdc:sty-11-aygzgv51?&max_page_size=100'
# resp = requests.get(og_url)
# data = resp.json()
# data = pd.DataFrame(data)
# del resp, og_url
# 
# #reformat data into dataframe (keeping biosample id)
# data_objects=[]
# for index, row in data.iterrows():
#     bio_id = row['biosample_id']
#     row_out = row.explode('data_objects').drop_duplicates()
#     row_out = pd.json_normalize(row_out)
#     row_out['biosample_id'] = bio_id
#     data_objects.append(row_out)
# 
# data_objects = pd.concat(data_objects)[['id','name','file_size_bytes','data_object_type','md5_checksum','url','biosample_id','in_manifest']]
# display(data_objects)
# 
# del data, index, row, row_out, bio_id


data_objects <- get_data_objects_for_study("nmdc:sty-11-aygzgv51") %>%
  # Remove unnecessary columns for simpler dataframe
  select(id, name, file_size_bytes, data_object_type, md5_checksum, url, biosample_id, in_manifest) %>%
  # Flatten in_manifest
  mutate(in_manifest = as.character(in_manifest))


```


Subset the data objects to 'Unfiltered Metaproteomic Results'. These files contain the proteomic workflow outputs that will be used for proteomic aggregation.

```{r}
# proteomic_output_df = data_objects[data_objects['data_object_type']=='Unfiltered Metaproteomics Results'].reset_index(drop=True).rename(columns={'id':'processed_DO_id'})
# display(proteomic_output_df)


proteomic_output_df <- data_objects %>%
  filter(data_object_type == "Unfiltered Metaproteomics Results") %>%
  dplyr::rename(processed_dobj_id = "id")

head(proteomic_output_df)

```

There are various requirements that enable mass spectrometry runs to be aggregated and analyzed together. For example, runs need to be performed in succession, on the same instrument. The NMDC schema can make it easier to find these proteomic results by linking them via a slot called `in_manifest`.

Look at the `in_manifest` id on these proteomic outputs to confirm that all runs are in the same manifest record, and pull that record. If that manifest record's `manifest_category` value is 'instrument_run', then it confirms that these are LC-MS/MS runs that were performed in succession on the same instrument. Proteomic outputs from different manifest records should not be aggregated.

```{r}
# #extract in_manifest ids for this study (in this case a single one available)
# manifest_id = proteomic_output_df.explode('in_manifest')['in_manifest'].unique()
# 
# #determine manifest_category
# manifest = api_func.get_id_results(newest_results=proteomic_output_df.to_dict(orient='records'), id_field = 'in_manifest', query_collection = 'manifest_set', match_id_field = 'id', query_fields = '')
# display(manifest)

# Display manifest IDs for the records in proteomic_output_df
manifest_id <- unique(proteomic_output_df$in_manifest)

# In this case there is only one, print manifest information
manifest <- get_results_by_id(collection = "manifest_set", 
                              match_id_field = "id", 
                              id_list = manifest_id, 
                              fields = "")
manifest

```

Look at an example of the information in 'Unfiltered Metaproteomics Results', which contains peptide identification and relative abundance information.

```{r}
# #example unfiltered results 
# unfilt_results = proteomic_output_df.iloc[0]["url"]
# print(unfilt_results)
# display(agg_func.tsv_extract(unfilt_results))
# 
# del unfilt_results

paste("Reading file from", proteomic_output_df$url[1])

head(read_tsv(proteomic_output_df$url[1], show_col_types = FALSE, progress = FALSE))


```

Extract information from all 33 proteomic results via the function iterate_file_extract() in agg_func, and put them into a single dataframe, where each scan in each dataset has the unique identifier `SpecID`. Clean prefix and suffix off of each peptide sequence. Since this data was processed using a target-decoy approach, determine the type of protein being matched to each peptide: contaminant, reverse (false positive match to the reversed amino acid sequence of a protein), or forward (match to the true, forward amino acid sequence of a protein). The presence of forward and reverse matches enables FDR estimation in the next step.

```{r define}

# Define functions

gff_extract_features <- function(url) {
  
  withCallingHandlers(
    expr = {
      
      tsv_df <- suppressWarnings(read_tsv(url, col_names = FALSE, progress = FALSE, show_col_types = FALSE))
      
      colnames(tsv_df) <- c("seqname", "source", "feature", "start", "end", "score", "strand", "frame", "attribute") 
      # See https://www.ensembl.org/info/website/upload/gff.html for GFF format specification
      
      # Break "attribute" column by separator
      tsv_df <- strsplit(tsv_df$attribute, split = ";") %>%
        lapply(strsplit, split = "=") %>%
        lapply(function (x) { 
          do.call(rbind, x) %>%
            t() %>%
            data.frame() %>%
            row_to_names(row_number = 1) }) %>%
        bind_rows() %>%
        distinct()
    },
    error = function(e) print(paste("Error while reading GFF from", url, ":", e))
  )
  return(tsv_df)
}



iterate_file_extract <- function(input_df, identifier_col, url_col, 
                                 extract_cols, file_type, 
                                 filter_col = NA, filter_values = NA) {
  
  output <- vector(mode = "list", length = nrow(input_df))

  for (row in 1:nrow(input_df)) {
    
    # Extract url and id for readability
    file_url <- input_df[[url_col]][row]
    identifier <- input_df[[identifier_col]][row]
    
    tryCatch(
      expr = {
        if(file_type == "tsv") {
          df <- read_tsv(file_url, show_col_types = FALSE, progress = FALSE)
          }
        if (file_type == "gff") {
          df <- gff_extract_features(file_url)
        }
      },
      error = function(e) print(paste("An error occurred fetching data from", identifier, ":", e))
    )
    
    # Check that the subsetted df will have unique rows, otherwise break
    if(nrow(df) != nrow(distinct(df[extract_cols]))) {
      print(paste("Selected columns result in non-unique rows for ", identifier, ". Data will not be included in output."))
      break
    }
    
    # Subset data frame to desired columns
    df <- df[extract_cols]
    
    # Filter if specified
    if (!is.na(filter_col) & all(is.na(filter_values))) {
      df <- filter(df, {{filter_col}} %in% filter_values)
    }
    
    # Add identifier column
    df <- mutate(df, id = identifier)
    
    # Append to list
    output[[row]] <- df
  }
  return(bind_rows(output))
}


# test_df <- data.frame(a = c(1, 2, 3, 4, 5),
#                       b = c(2, 3, 5, 6, 6),
#                       xyz = c(1, 1, 1, 1, 1),
#                       abc = c(2, 2, 2, 2, 2),
#                       url = proteomic_output_df$url[1:5])
# 
# 
# q <- iterate_file_extract(input_df = test_df,
#                      identifier_col = "a",
#                      url_col = "url",
#                      extract_cols = c('Charge','Scan','Peptide','Protein','MSGFDB_SpecEValue','StatMomentsArea'),
#                      file_type = "tsv")



trim_peptide_sequence <- function(s) {
  str_match(s, "\\.([A-Z\\\\*@#]+)\\.")[, 2]
}

#trim_peptide_sequence("ASDF.FGHJKL.QWERTY")

```

```{r}
# unfilt_res = agg_func.iterate_file_extract(identifier_col='processed_DO_id',
#                                 url_col='url',
#                                 extract_cols=['Charge','Scan','Peptide','Protein','MSGFDB_SpecEValue','StatMomentsArea'],
#                                 pd_df=proteomic_output_df,
#                                 filter_col = None,
#                                 filter_values = None,
#                                 file_type='tsv'
#                                 )




unfiltered_results <- iterate_file_extract(input_df = proteomic_output_df,
                                           identifier_col = "processed_dobj_id",
                                           url_col = "url", 
                                           extract_cols = c("Charge", "Scan", "Peptide", "Protein", "MSGFDB_SpecEValue", "StatMomentsArea"),
                                           file_type = "tsv") %>%


# #create identifier for each scan in each dataset
# unfilt_res["SpecID"] = unfilt_res.apply(lambda row: str(row["id_col"]) + "_" + str(row["Scan"]), axis=1)


  mutate(SpecID = paste(id, Scan, sep = "_")) %>%

# #clean off the prefix and suffix from the sequence but keep any mods
# unfilt_res["Peptide Sequence with Mods"] = unfilt_res["Peptide"].apply(agg_func.sequence_noprefsuff)
# del unfilt_res['Peptide']




  mutate(Peptide_Sequence_with_Mods = trim_peptide_sequence(Peptide)) %>%



# #determine protein type (contaminant, reverse, forward)
# unfilt_res["Protein_Type"] = unfilt_res["Protein"].apply(agg_func.findproteinname)

  mutate(Protein_Type = case_when(
    str_detect(Protein, "Contaminant") ~ "None",
    str_detect(Protein, "^XXX_") ~ "Reversed",
    TRUE ~ "Forward"))

# unfilt_res

head(unfiltered_results)

```

## 2) Apply a spectral probability filter across the data that optimizes the number of identifications for an FDR of 0.05

A challenge associated with aggregating mass spectrometry data is that there are always false identifications, which can be mitigated by imposing a spectral probability filter on the data being analyzed. The same spectral probability filter needs to be applied across datasets when they are being compared. The filter value itself is chosen by weighing the number of 'true' identifications retained with the proximity of the data to a chosen false discovery rate (FDR) (usually 0.05 or 0.01). NMDC's metaproteomic workflow provides 'true' and 'false' identifications for FDR estimation in the 'Unfiltered Metaproteomic Result' files.
Create a dataframe of peptide identifications (ignoring protein mapping). Filter identifications to the peptide sequence with the smallest SpecEValue for each SpecID, so there is a single, highest probability identification for each scan.

```{r}
# edata = unfilt_res[['SpecID','Peptide Sequence with Mods','MSGFDB_SpecEValue','Protein_Type','StatMomentsArea']].drop_duplicates()  # important to remove any duplicated rows here!

edata <- distinct(unfiltered_results, SpecID, Peptide_Sequence_with_Mods, MSGFDB_SpecEValue, Protein_Type, StatMomentsArea, .keep_all = TRUE)



#for each SpecID, select the peptide spectrum match with the smallest MSGFDB_SpecEValue (.idxmin() takes the first entry when there's multiple matches)
# idx = edata.groupby(['SpecID'])['MSGFDB_SpecEValue'].idxmin()
# edata = edata.loc[idx].reset_index(drop=True)
# del idx

edata <- edata %>% 
  group_by(SpecID) %>% 
  slice_min(MSGFDB_SpecEValue, with_ties = FALSE, n = 1) %>% 
  ungroup()



# display(edata)

head(edata)

# assert len(edata['SpecID'].unique())==edata.shape[0], "still more than one identification per scan"

stopifnot("Still more than one identification per scan" = length(unique(edata$SpecID)) == length(edata$SpecID))

```

Create separate dataframes of forward and reverse peptide spectrum matches.

```{r}
# forward_peptides = edata[edata["Protein_Type"] == "Forward"].copy().reset_index(drop=True)
# del forward_peptides["Protein_Type"]
# display(forward_peptides)

forward_peptides <- filter(edata, Protein_Type == "Forward") %>% select(-Protein_Type)

head(forward_peptides)

```

```{r}
# reversed_peptides = edata[edata["Protein_Type"] == "Reversed"].copy().reset_index(drop=True)
# del reversed_peptides["Protein_Type"]
# del edata
# display(reversed_peptides)

reversed_peptides <- filter(edata, Protein_Type == "Reversed") %>% select(-Protein_Type)

head(reversed_peptides)

# rm(edata)

```

Use the function optimize_specFilt() in agg_func to find a log10 spectral probability filter that weighs the number of forward peptides retained with the proximity of the dataset to a 0.05 spectral FDR. Visualize the impact of the spectral probability filter by plotting the number of forward and reverse peptides retained. 

The main plot below is a histogram of forward and reverse peptides across all spectral probability values. The inset within this plot depicts a subset of the smallest spectral probabililty values, with the red bar before the dashed line representing the estimated number of false identifications that will be included in this analysis.

```{r}

#initial guess at a log10 spectral probability filter value
initial_specprob_filter = -15


spec_filt_value <- function(specprob, forward_peptides, reversed_peptides) {

  df_r <- filter(reversed_peptides, MSGFDB_SpecEValue < 10 ^ specprob)
  df_f <- filter(forward_peptides, MSGFDB_SpecEValue < 10 ^ specprob)

  f_spec <- length(unique(df_f$SpecID))
  r_spec <- length(unique(df_r$SpecID))
  
  fdr_spec <- ifelse(f_spec == 0 & r_spec == 0,
                     1,
                     (2 * r_spec) / (f_spec + r_spec))
    
  filter_value <- 1 / (0.050001 - fdr_spec) * (-f_spec)
  
  return(filter_value)
}





optimize_spec_filt <- function(initial_specprob_filter, forward_peptides, reversed_peptides) {

 result <- optim(fn = spec_filt_value, par = initial_specprob_filter, 
          forward_peptides = forward_peptides, reversed_peptides = reversed_peptides,
           method = "Brent", lower = -100, upper = 100)
 
 # Brent method for optim() function always returns 0 (success) for convergence
 
  tryCatch(
    expr = {
      result <- optim(fn = spec_filt_value, par = initial_specprob_filter, 
                      reversed_peptides = reversed_peptides, forward_peptides = forward_peptides,
                      method = "Brent", lower = -100, upper = 100)
      },
    error = function(e) message(paste("Error in optimization:", e)),
    warning = function(w) message(paste("Warning in optimization:", w))
  )
}

optimized_filter <- optimize_spec_filt(initial_specprob_filter, forward_peptides, reversed_peptides)$par




```

```{r filter-plot, eval = FALSE}

#visualize filter
fitted_params=optimization.x
fig, ax_main = plt.subplots()
ax_inset = plt.axes([0.45, 0.45, 0.35, 0.35])


# Main plot
# forward peptides
hist, bins = np.histogram(forward_peptides["MSGFDB_SpecEValue"], bins=50)
ax_main.bar(bins[:-1], hist, width=np.diff(bins), align='edge', color='g', label='forward')

# reverse peptides
hist, bins = np.histogram(reversed_peptides["MSGFDB_SpecEValue"], bins=50)
ax_main.bar(bins[:-1], hist, width=np.diff(bins), align='edge',color='r', alpha=0.7, label='reverse')

# filter cutoff
ax_main.axvline(x=10 ** fitted_params[0], color="black", label = 'filter cutoff', linestyle="--")
ax_main.set_ylabel('number of peptide spectrum matches')
ax_main.set_xlabel('MSGFDB_SpecEValue')
ax_main.ticklabel_format(style='plain', axis='x')
ax_main.set_title(f'impact of spectral probability filter {10 ** fitted_params[0]}')
ax_main.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)


# Inset plot
# forward peptides
hist, bins = np.histogram(forward_peptides[forward_peptides["MSGFDB_SpecEValue"] < 0.000000002]["MSGFDB_SpecEValue"], bins=18)
ax_inset.bar(bins[:-1], hist, width=np.diff(bins), align='edge', color='g')

# reverse peptides
hist, bins = np.histogram(reversed_peptides[reversed_peptides["MSGFDB_SpecEValue"] < 0.000000002]["MSGFDB_SpecEValue"], bins=18)
ax_inset.bar(bins[:-1], hist, width=np.diff(bins), align='edge',color='r', alpha=0.7)

# filter cutoff
ax_inset.axvline(x=10 ** fitted_params[0], color="black", label = 'filter cutoff', linestyle="--")
ax_inset.set_xlabel('MSGFDB_SpecEValue')

plt.show
```


```{r}

peps_for_plot <- bind_rows(forward_peptides, reversed_peptides, .id = "direction") %>%
  mutate(direction = case_when(direction == 1 ~ "forward", direction == 2 ~ "reverse"),
         direction = factor(direction))

main_plot <- ggplot(peps_for_plot) +
  geom_histogram(aes(x = MSGFDB_SpecEValue, fill = direction), bins = 50, alpha = 0.5, position = "identity") + 
  geom_vline(xintercept = 10 ^ optimized_filter) +
  scale_fill_manual(values = c("forward" = "seagreen", "reverse" = "orangered")) +
  ylab("Number of peptide-spectrum matches") +
  ggtitle("Impact of spectral probability filter")

zoom_plot <- peps_for_plot %>%
  # subset data - zoom in 
  filter(MSGFDB_SpecEValue < 2e-9) %>%
  ggplot() +
    geom_histogram(aes(x = MSGFDB_SpecEValue, fill = direction), bins = 30, alpha = 0.5, position = "identity") + 
    geom_vline(xintercept = 10 ^ optimized_filter) +
    scale_fill_manual(values = c("forward" = "seagreen", "reverse" = "orangered")) +
    theme(legend.position = "none", axis.title.x = element_blank(), axis.title.y = element_blank())
    
vp <- viewport(width = 0.4, height = 0.45, x = 0.6, y = 0.65)

main_plot
print(zoom_plot, vp = vp)


# rm(peps_for_plot, main_plot, zoom_plot, vp)
```

Apply the filter to the dataset and recalculate peptide and spectral FDR.

```{r}
#Filter the data according to the filter and recalculate FDR
# forward_peptides = forward_peptides[
#     (forward_peptides["MSGFDB_SpecEValue"] < 10 ** optimization.x[0])
# ].copy()


forward_peptides <- filter(forward_peptides, MSGFDB_SpecEValue < 10 ^ optimized_filter)

# reversed_peptides = reversed_peptides[
#     (reversed_peptides["MSGFDB_SpecEValue"] < 10 ** optimization.x[0])
# ].copy()

reversed_peptides <- filter(reversed_peptides, MSGFDB_SpecEValue < 10 ^ optimized_filter)


# Calculate FDR
# f_spec = (forward_peptides["SpecID"].unique().size)
# r_spec = reversed_peptides["SpecID"].unique().size

f_spec <- length(unique(forward_peptides$SpecID))
r_spec <- length(unique(reversed_peptides$SpecID))


# if (f_spec == 0) & (r_spec == 0):
#     fdr_spec = 1
# else:
#     fdr_spec = (2*r_spec) / (f_spec + r_spec)

fdr_spec <- ifelse(f_spec == 0 & r_spec == 0,
                   1,
                   (2 * r_spec) / (f_spec + r_spec))

# f_pep = forward_peptides["Peptide Sequence with Mods"].unique().size
# r_pep = reversed_peptides["Peptide Sequence with Mods"].unique().size

f_pep <- length(unique(forward_peptides$Peptide_Sequence_with_Mods))
r_pep <- length(unique(reversed_peptides$Peptide_Sequence_with_Mods))


# if (f_pep == 0) & (r_pep == 0):
#     fdr_pep = 1
# else:
#     fdr_pep = (r_pep) / (f_pep + r_pep)


fdr_pep <- ifelse(f_pep == 0 & r_pep == 0,
                  1,
                  r_pep / (f_pep + r_pep))

# print("Spectral FDR:",fdr_spec,"\nPeptide FDR:",fdr_pep)


paste("Spectral FDR:", fdr_spec)
paste("Peptide FDR:", fdr_pep)


# rm(f_spec, r_spec, f_pep, r_pep)
```

## 3) Collapse to unique peptides and normalize their relative abundance

At this point in analysis the data has been filtered to only high probability peptide identifications, but more than one scan within a dataset can have the same peptide identification. This can be due to the peptide eluting into the mass spectrometer over the course of multiple scans or a peptide eluting as multiple charge states. Sum the relative abundance for peptide sequences detected more than once in a dataset, leaving a total relative abundance value for each peptide in each dataset.

```{r}
#extract data set id
# forward_peptides['processed_DO_id'] = forward_peptides['SpecID'].str.split('_').str[0]

forward_peptides <- forward_peptides %>%
  dplyr::rename(processed_dobj_id = id) %>%
  select(-c(SpecID, MSGFDB_SpecEValue)) %>%
  group_by(processed_dobj_id, Peptide_Sequence_with_Mods) %>%
  mutate(StatMomentsArea = sum(StatMomentsArea)) %>% 
  ungroup() %>%
  distinct(processed_dobj_id, Peptide_Sequence_with_Mods, StatMomentsArea)

head(forward_peptides)


#drop SpecID and spectral probability columns since no longer relevant
# forward_peptides.drop(['SpecID','MSGFDB_SpecEValue'],axis=1, inplace=True)

#for each peptide sequence with mods, sum the abundances for all scans/identifications
# forward_peptides = forward_peptides.groupby(['processed_DO_id','Peptide Sequence with Mods'])['StatMomentsArea'].sum().to_frame().reset_index()
# 
# display(forward_peptides)
```

Visualize the untransformed and un-normalized relative abundances.

```{r, eval = FALSE}
#plot untransformed abundance values
untransformed_abundances_fig, ax = plt.subplots(figsize=(8,6))
sns.boxplot(x='processed_DO_id',y='StatMomentsArea',data=forward_peptides)
plt.ticklabel_format(style='plain', axis='y')
plt.xticks([])
plt.xlabel('sample')
plt.ylabel('Relative Peptide Abundance (Un-Normalized)')


ggplot(forward_peptides) +
  geom_boxplot(aes(x = processed_dobj_id, y = StatMomentsArea)) +
  labs(x = "Samples", y = "Relative Peptide Abundance (Not Normalized)", title = "Peptide relative abundances by sample") +
  theme(axis.text.x = element_blank())






```

Apply log2 transformation and median normalize peptide abundances.

```{r}
#log2 tranformation
# forward_peptides['StatMomentsAreaLog2']=np.log2(forward_peptides['StatMomentsArea'])

# Calculate group-wise (sample wise) median
# group_medians = forward_peptides.groupby('processed_DO_id')['StatMomentsAreaLog2'].median()

forward_peptides <- forward_peptides %>%
  mutate(StatMomentsAreaLog2 = log2(StatMomentsArea)) %>%
  group_by(processed_dobj_id) %>%
  mutate(group_medians = median(StatMomentsAreaLog2)) %>%
  ungroup() %>%
  distinct()


# Calculate data wide median
# all_data_median=forward_peptides['StatMomentsAreaLog2'].median()

all_data_median <- median(forward_peptides$StatMomentsAreaLog2)

# Subtract sample wise median from each value within its group
# forward_peptides['StatMomentsAreaLog+Norm'] = forward_peptides.apply(
#     lambda row: row['StatMomentsAreaLog2'] - group_medians[row['processed_DO_id']], axis=1
# )

#add back in a data wide median value to avoid negative abundances
# forward_peptides['StatMomentsAreaLog+Norm'] = forward_peptides['StatMomentsAreaLog+Norm'] + all_data_median


forward_peptides <- forward_peptides %>%
  mutate(StatMomentsAreaLogNorm = StatMomentsAreaLog2 - group_medians + all_data_median)



# transformed_abundances_fig, ax = plt.subplots(figsize=(8,6))
# sns.boxplot(x='processed_DO_id',y='StatMomentsAreaLog+Norm',data=forward_peptides)
# plt.xticks([])
# plt.xlabel('sample')
# plt.ylabel('Relative Peptide Abundance (Normalized)')
# 
# del ax, group_medians, all_data_median, forward_peptides['StatMomentsArea'], forward_peptides['StatMomentsAreaLog2'], reversed_peptides


ggplot(forward_peptides) +
  geom_boxplot(aes(x = processed_dobj_id, y = StatMomentsAreaLogNorm)) +
  labs(x = "Samples", y = "Relative Peptide Abundance (Normalized)", title = "Peptide relative abundances by sample") +
  theme(axis.text.x = element_blank())


```

## 4) Extract functional gene annotations for proteins

Collect peptide to protein mapping information for the passing peptide sequences.

```{r}
# peptide_protein_mapping = pd.DataFrame(unfilt_res[unfilt_res['Peptide Sequence with Mods'].isin(forward_peptides['Peptide Sequence with Mods'])][['Peptide Sequence with Mods','Protein']].drop_duplicates()).reset_index(drop=True)
# peptide_protein_mapping

peptide_protein_mapping <- unfiltered_results %>%
  filter(Peptide_Sequence_with_Mods %in% forward_peptides$Peptide_Sequence_with_Mods) %>%
  distinct(Peptide_Sequence_with_Mods, Protein)

peptide_protein_mapping
```

Annotation information for these proteins can be found in 'Functional Annotation GFF' files.

Since the `data_objects` dataframe contains all objects associated with our study id, it also contains the relevant 'Functional Annotation GFF' files. Subset this dataframe to GFF files associated with the 33 biosample ids that have a proteomic output in `proteomic_output_df`.

```{r}
# annotation_input_df = data_objects[(data_objects['data_object_type']=='Functional Annotation GFF') & (data_objects['biosample_id'].isin(proteomic_output_df['biosample_id'].unique().tolist()))].reset_index(drop=True)

# annotation_input_df = annotation_input_df[['id','file_size_bytes','data_object_type','md5_checksum','url','biosample_id']]

annotation_input_df <- data_objects %>%
  filter(data_object_type == "Functional Annotation GFF") %>%
  filter(biosample_id %in% proteomic_output_df$biosample_id) %>%
  distinct(biosample_id, id, data_object_type, url)

# display(annotation_input_df)

head(annotation_input_df)
```

Preview the 'Functional Annotation GFF' files and determine a subset of gene annotation information that should be pulled from all 33 files.

```{r}
#example annotation file





paste("Reading from", annotation_input_df$url[2])

head(gff_extract_features(annotation_input_df$url[2]))

```

Extract information from all 33 annotation files (this takes a while to run) and merge with `razormapping` so there is a final table of peptide-protein-annotation mapping (`annotation_mapping`).

```{r}
#get gene annotation information for proteins
# genemapping = annotation_input_df[["id", "url"]].drop_duplicates().reset_index(drop=True)
gene_mapping <- distinct(annotation_input_df, id, url)

# genemapping = agg_func.iterate_file_extract(
#     pd_df=genemapping,\
#     identifier_col='id',\
#     url_col='url',\
#     extract_cols=['ID','product','product_source'],\
#     filter_col = 'ID',
#     filter_values = peptide_protein_mapping['Protein'].unique().tolist(),
#     file_type='gff'
# )


gene_mapping <- iterate_file_extract(
  input_df = gene_mapping,
  identifier_col = "id",
  url_col = "url",
  extract_cols = c("ID", "product", "product_source"),
  filter_col = "ID",
  filter_values = unique(peptide_protein_mapping$Protein),
  file_type = "gff"
)


#merge with protein mapping information. drop columns ID (which is equivalent to Protein) and id_col (which is the dataset id, unnecessary here since peptide to protein information isn't dataset specific)
# annotation_mapping = genemapping.merge(peptide_protein_mapping,left_on='ID',right_on='Protein').drop(['ID','id_col'],axis=1)

annotation_mapping <- inner_join(gene_mapping, peptide_protein_mapping, by = join_by(ID == Protein)) %>%
  distinct()

annotation_mapping
```

## 5) Generate annotation and protein mappings for peptides using "Razor" strategy

Identify the razor protein, which is a method of limiting the assignment of degenerate peptides (i.e., peptides that map to more than one forward protein) to a most likely matched protein. 'Razor' references the principle Occam's razor, also known as the law of parsimony.

The rules are as follows:
- If a peptide is unique to a protein, then that protein is the razor
- Else, if a peptide belongs to more than one protein, but one of those proteins has a unique peptide, then that protein is the razor
- Else, if a peptide belongs to more than one protein and one of those proteins has the maximal number of peptides, then that protein is the razor
- Else, if a peptide belongs to more than one protein and more than one of those proteins has the maximal number of peptides, then collapse the proteins and gene annotations into single strings
- Else, if a peptide belongs to more than one protein and more than one of those proteins has a unique peptide, then the peptide is removed from analysis because its mapping is inconclusive

Use `annotation_mapping` as the input to the function razorprotein() from the agg_func script. This will return protein and gene annotation information for each peptide, according to the above rules. 

```{r}

# Add counts for use in razor logic function
# annotation_mapping has already been through distinct() so pairs (rows) are unique
annotation_mapping <- annotation_mapping %>%
  
  dplyr::rename(Protein = ID) %>%
  
  # Count the number of proteins that each peptide maps to
  group_by(Peptide_Sequence_with_Mods) %>%
  mutate(prot_count = n()) %>%
  ungroup() %>%
  
  # Count the number of REDUNDANT and UNIQUE peptides that each protein maps to
  group_by(Protein) %>%
  mutate(redundant_pep_count = sum(prot_count > 1),
         unique_pep_count = sum(prot_count == 1)) %>%
  ungroup()

annotation_mapping

```

```{r}


get_razor_protein <- function(mapping_df) {
    
  # Make the mapping df into a bunch of named vectors for easier indexing
  
  # Names = peptides, values = number of proteins the peptide maps to
  a <- distinct(mapping_df, Peptide_Sequence_with_Mods, prot_count)
  prot_count_vec <- setNames(a$prot_count, a$Peptide_Sequence_with_Mods)
  
  # Names = peptides, values = proteins
  pep_prot_vec <- setNames(mapping_df$Protein, nm = mapping_df$Peptide_Sequence_with_Mods)
  
  # Create vector of all peptides for readability
  pep_vec <- a$Peptide_Sequence_with_Mods
  rm(a)
  

  # Pre allocate results list
  razor_result <- vector(mode = "list", length = length(pep_vec))
  
  # Iterate through peptides to choose a razor protein for each one
  for (pep in 1:length(pep_vec)) {
    
    query_peptide <- pep_vec[pep]
    
    # If the peptide only maps to one protein, that is the razor protein
    if (prot_count_vec[query_peptide] == 1) { razor_result[[pep]] <- pep_prot_vec[query_peptide] }
    
    # If the peptide maps to more than one protein and ...
    else {
      mapping_subset <- filter(mapping_df, Peptide_Sequence_with_Mods == query_peptide)
      prots_with_unique_peptides <- sum(mapping_subset$unique_pep_count > 0)
      
      razor_result[[pep]] <- case_when(
        
        # ...there is only one potential protein with unique peptides, that is the razor protein
        prots_with_unique_peptides == 1 ~ mapping_subset$Protein[which.max(mapping_subset$unique_pep_count)],
        
        # ...there is more than one potential protein with unique peptides, razor protein cannot be determined
        prots_with_unique_peptides > 1  ~ "indeterminate - discard",
        
        # ...there are no potential proteins with unique peptides, 
        # and ONE potential protein has the most redundant peptides, that is the razor protein
        # note: which.max returns the FIRST maximum index which in this case should be the only one
        prots_with_unique_peptides == 0 & 
          sum(mapping_subset$redundant_pep_count == max(mapping_subset$redundant_pep_count)) == 1 ~ 
          mapping_subset$Protein[which.max(mapping_subset$redundant_pep_count)],
        
        # ...there are no potential proteins with unique peptides, 
        # and more than one potential protein has the most redundant peptides, those are the razor proteins
        # note: which ( blah == max(blah) ) will return ALL of the maximum indices
        prots_with_unique_peptides == 0 &
          sum(mapping_subset$redundant_pep_count == max(mapping_subset$redundant_pep_count)) > 1 ~ 
          mapping_subset$Protein[which(mapping_subset$redundant_pep_count == max(mapping_subset$redundant_pep_count))],
        
        # there should be no cases not captured
        TRUE ~ "fix razor logic until you don't see this"
      )
    }
    razor_result[[pep]] <- data.frame(Peptide = query_peptide,
                                      Razor_Protein = razor_result[[pep]], 
                                      row.names = NULL)
  }
  # Bind into one long dataframe
  bind_rows(razor_result, .id = NULL) %>%
    
    # Discard indeterminate cases
    filter(Razor_Protein != "indeterminate - discard") %>%
    
    # Add the protein annotations back in
    left_join(select(mapping_df, Protein, product, product_source, Peptide_Sequence_with_Mods),
              by = join_by(Razor_Protein == Protein, Peptide == Peptide_Sequence_with_Mods)) %>%
    distinct()
}

razor_mapping_long <- get_razor_protein(annotation_mapping)

razor_mapping_oneline <- razor_mapping_long %>%
  group_by(Peptide) %>%
  mutate(Razor_Protein  = paste(Razor_Protein, collapse = ", "),
         product        = paste(product, collapse = ", "),
         product_source = paste(product_source, collapse = ", ")) %>%
  ungroup() %>%
  distinct()

razor_mapping_oneline

```

## 6) Perform protein rollup and summarize into a final aggregated table of relative protein abundance

Combine razor information with relative abundance values.

```{r}
# #merge needs to be 'right' because some peptides are removed in mapping functions if they have indeterminante mappings
# forward_peptides = forward_peptides.merge(razormapping,how='right',on=['Peptide Sequence with Mods'])
# del annotation_mapping, peptide_protein_mapping, genemapping
# 
# forward_peptides





forward_peptides <- forward_peptides %>%
  right_join(razor_mapping_oneline, by = join_by(Peptide_Sequence_with_Mods == Peptide)) %>%
  distinct()

forward_peptides

```

De-log the peptide abundances, sum the abundances for each razor protein and log transform the rolled up protein abundances.

```{r}
# #de-log abundance
# forward_peptides['StatMomentsAreaNorm']=2**forward_peptides['StatMomentsAreaLog+Norm']
# forward_peptides.drop('StatMomentsAreaLog+Norm',axis=1,inplace=True)
# 
# 
# 
# 
# 
# 
# 
# 
# #sum abundance for each sorted_list protein in each dataset
# prot_info = forward_peptides.columns[~forward_peptides.columns.isin(['StatMomentsAreaNorm','Peptide Sequence with Mods'])].tolist()
# protein_abundances = forward_peptides.groupby(prot_info)['StatMomentsAreaNorm'].sum().reset_index()
# 
# #re-log2 tranform abundance
# protein_abundances['StatMomentsAreaLog+Norm']=np.log2(protein_abundances['StatMomentsAreaNorm'])
# protein_abundances.drop('StatMomentsAreaNorm',axis=1,inplace=True)
# 
# del prot_info, forward_peptides


protein_abundances <- forward_peptides %>%
  mutate(StatMomentsAreaNorm = 2 ^ StatMomentsAreaLogNorm) %>%
  group_by(processed_dobj_id, Razor_Protein) %>%
  mutate(StatMomentsAreaNormSum = sum(StatMomentsAreaNorm)) %>%
  ungroup() %>%
  distinct(processed_dobj_id, product, product_source, Razor_Protein, StatMomentsAreaNormSum) %>%
  mutate(StatMomentsAreaLogNormSum = log2(StatMomentsAreaNormSum)) %>%
  select(-StatMomentsAreaNormSum)


protein_abundances
```

## Final aggregated table of relative protein abundance

Reformat these results into a proteomic table, where each row indicates a protein and each column indicates a sample/dataset. The values within are log transformed, median normalized relative abundance values. This table or the longform version above can be used in further proteomic analyses.

```{r}
#pivot to wide crosstab
# aggregated_proteomic_output = protein_abundances.pivot(index='razor',columns='processed_DO_id',values='StatMomentsAreaLog+Norm')
# aggregated_proteomic_output.columns.name = None
# 
# aggregated_proteomic_output


aggregated_proteomic_output <- protein_abundances %>%
  select(processed_dobj_id, Razor_Protein, StatMomentsAreaLogNormSum) %>%
  pivot_wider(names_from = "processed_dobj_id", values_from = "StatMomentsAreaLogNormSum")


aggregated_proteomic_output

```

The generated protein table can be used as input to the software [pmartR](https://shinyproxy.emsl.pnnl.gov/app/pmart), which performs statistical analyses such as ANOVA and independence of missing data (IMD) tests. In this case, the aggregated proteomics table (`aggregated_proteomic_output`) would be equivalent to pmartR's `e_data` and the long form peptide to protein to gene mappings (`razor_mapping_long`) would be equivalent to pmartR's `e_meta`.

<img src="../pmartR_logo_final.jpg" width="25%"/>

pmartR requires sample metadata to parameterize analyses and interpret the data. For this example dataset, an API call will capture the biosample metadata (`sample_metadata`) that would be equivalent to pmartR's `f_data`.

Gather biosample metadata via the NMDC API `collection` endpoint, using the function get_id_results() in api_func and searching for the `biosample_id`s associated with each output in `proteomic_output_df`.

```{r}
# #collect biosample metadata via the API
# biosamples = api_func.get_id_results(newest_results = proteomic_output_df.to_dict(orient='records'), 
#                                      id_field = 'biosample_id', 
#                                      query_collection = 'biosample_set', 
#                                      match_id_field = 'id', 
#                                      query_fields = 'id,depth.has_numeric_value')
# 
# #normalize the json columns
# biosamples = pd.json_normalize(biosamples).drop_duplicates().rename(columns={'id':'biosample_id'})
# 
# #merge biosample and processed data object information
# sample_metadata = biosamples.merge(proteomic_output_df[['processed_DO_id','biosample_id']],on='biosample_id')
# del biosamples
# 
# sample_metadata

biosample_metadata <- get_results_by_id(collection = "biosample_set",
                                        match_id_field = "id", 
                                        id_list = proteomic_output_df$biosample_id, 
                                        fields = "id,depth.has_numeric_value") %>%
  # Cleanup json output
  unnest(depth) %>%
  dplyr::rename(biosample_id = id,
                depth_m = has_numeric_value) %>%
  # Add data object IDs to connect biosample metadata to processed results
  left_join(select(proteomic_output_df, processed_dobj_id, biosample_id), by = join_by("biosample_id"))

biosample_metadata

```

